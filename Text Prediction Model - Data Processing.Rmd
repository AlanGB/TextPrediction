---
title: "Text Prediction Model - Analyzing the Data"
author: "Alán García Bernal"
date: "12/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## Summary

The goal of this project is to create a Text Prediction Model (for the English Language) using the [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) given by Coursera. In this report you will find an Overview of the data, as well as the steps taken for cleaning and processing the data. Lastly, there are the N-gram Models that will be used for creating the final product.

## Data Overview

After downloading and extracting the datasets we are interested in, which are a file with **Twitter**, **Blogs** and **News** entries, we can start seeing interesting aspects of the data.

```{r data loading, cache=FALSE}
library(readr)
twitter.file <- ("./final/en_US/en_US.twitter.txt")
blogs.file <- ("./final/en_US/en_US.blogs.txt")
news.file <- ("./final/en_US/en_US.news.txt")

twitter <- read_lines(twitter.file)
blogs <- read_lines(blogs.file)
news <- read_lines(news.file)
```

Here we can see the files are fairly large, but have about the same word count. Also, as expected, Twitter is the file with more lines but less Average words, because of the character limit per twitt. Blogs is the file with more words per register, followed by news.

```{r data overvew, warning=FALSE, cache=FALSE}
library(stringi)
rows <- c("Twitter", "Blogs", "News")

Size.MB <- setNames(as.data.frame(
                        file.size(twitter.file, blogs.file, news.file)/1024,
                        row.names = rows),
                    c("Size.MB"))

Rows <- setNames(as.data.frame(c(length(twitter), length(blogs), length(news)),
                               row.names = rows),
                 c("Rows"))

Words <- setNames(as.data.frame(c(sum(stri_count_words(twitter)),
                         sum(stri_count_words(blogs)),
                         sum(stri_count_words(news))),
                         row.names= rows),
                  c("Total Words"))

Words.Ave <- setNames(as.data.frame(c(mean(stri_count_words(twitter)),
                                      mean(stri_count_words(blogs)),
                                      mean(stri_count_words(news))),
                                    row.names = rows),
                      c("Average Words"))

data.overview <- cbind(Size.MB, Rows, Words, Words.Ave)
```

```{r data overview table, echo = FALSE, warning=FALSE, cache=FALSE}
library(formattable)
data.overview$`Average Words` <- format(round(as.numeric(data.overview$`Average Words`),digits = 0), nsmall = 0)
data.overview$`Total Words` <- format(as.numeric(data.overview$`Total Words`), nsmall = 0, big.mark =",")
data.overview$Rows <- format(as.numeric(data.overview$Rows), nsmall=0, big.mark=",")
data.overview$Size.MB <- format(round(as.numeric(data.overview$Size.MB), 1), nsmall=1, big.mark=",")


formattable(data.overview,
            align = c("l", "c", "c", "c", "c"))
```

## Data Sampling

Since the dataset is so large, and due to hardware constrains, we will be using the 15% of the total data. We will join this subsets into one *coropra* and divide them to create three data sets: the **training set**, **validation set** and the **test set**. As a good practice, we will use the 70% of the *corpora* as our **training set**, 20% for our **validation set** and the other 10% as our **test set**.

For practicity we will create separte testing, validation and training files for each one of the **Twitter**, **Blogs** and **News** data sets.

#### Twitter files
```{r twitter sampling, eval=FALSE}
set.seed(123)
con <- file("./final/en_US/en_US.twitter.txt", "r")
for (i in 1:n.twitter) {
        if (rbinom(1,1,.15) == 1) {
                if (rbinom(1,1, .7) == 1) {
                        write(readLines(con,1), file = "./final/en_US/train_US.twitter.txt", append = TRUE)     
                } else {
                        if (rbinom(1,1, .2) == 1) {
                                write(readLines(con,1), file = "./final/en_US/validation_US.twitter.txt", append = TRUE)
                        } else {
                                write(readLines(con, 1), file = "./final/en_US/test_US.twitter.txt", append = TRUE)
                        }
                }
                
        }
}
```

#### Blogs files
```{r blogs sampling, eval=FALSE}
set.seed(456)
con <- file("./final/en_US/en_US.blogs.txt", "r")
for (i in 1:n.blogs) {
        if (rbinom(1,1,.15) == 1) {
                if (rbinom(1,1, .7) == 1) {
                        write(readLines(con,1), file = "./final/en_US/train_US.blogs.txt", append = TRUE)     
                } else {
                        if (rbinom(1,1, .2) == 1) {
                                write(readLines(con,1), file = "./final/en_US/validation_US.blogs.txt", append = TRUE)
                        } else {
                                write(readLines(con, 1), file = "./final/en_US/test_US.blogs.txt", append = TRUE)
                        }
                }
                
        }
}
close(con)
```

#### News files
```{r news sampling, eval=FALSE}
news <- read_lines("./final/en_US/en_US.news.txt")
set.seed(789)
for (i in 1:length(news)) {
        if(rbinom(1,1,.15) == 1) {
                if (rbinom(1,1,.7) == 1) {
                        write(news[i], file = "./final/en_US/train_US.news.txt", append = TRUE)
                } else {
                        if (rbinom(1,1,.2) == 1) {
                                write(news[i], file = "./final/en_US/validation_US.news.txt", append = TRUE)
                        } else {
                                write(news[i], file = "./final/en_US/test_US.news.txt", append = TRUE)
                        }
                }
        }
}
rm(list = "news")
```

#### Final Datasets

```{r training set, eval=FALSE, echo = FALSE}
# training file
con <- file("./final/en_US/train_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/train_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/train_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

# training file
con <- file("./final/en_US/validation_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/validation_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/validation_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

# testing file
con <- file("./final/en_US/test_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/test_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/test_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)
```

Now, we can make the same general analysis on our training set, and we cn see that in all values, except for one, we have about 10% of the values for the complete sets. This is because we are using about 9.5% of the data. However, if we look at the average words per sete, we can see it is almost the same, so we could say the sample resembles the original.

```{r training data overview, echo = FALSE}
library(stringi)
library(formattable)

twitter.file <- ("./final/en_US/train_US.twitter.txt")
blogs.file <- ("./final/en_US/train_US.blogs.txt")
news.file <- ("./final/en_US/train_US.news.txt")

twitter <- read_lines(twitter.file)
blogs <- read_lines(blogs.file)
news <- read_lines(news.file)


rows <- c("Twitter", "Blogs", "News")

Size.MB <- setNames(as.data.frame(
        file.size(twitter.file, blogs.file, news.file)/1024,
        row.names = rows),
        c("Size.MB"))

Rows <- setNames(as.data.frame(c(length(twitter), length(blogs), length(news)),
                               row.names = rows),
                 c("Rows"))

Words <- setNames(as.data.frame(c(sum(stri_count_words(twitter)),
                                  sum(stri_count_words(blogs)),
                                  sum(stri_count_words(news))),
                                row.names= rows),
                  c("Total Words"))

Words.Ave <- setNames(as.data.frame(c(mean(stri_count_words(twitter)),
                                      mean(stri_count_words(blogs)),
                                      mean(stri_count_words(news))),
                                    row.names = rows),
                      c("Average Words"))

train.data.overview <- cbind(Size.MB, Rows, Words, Words.Ave)

train.data.overview$`Average Words` <- format(round(as.numeric(train.data.overview$`Average Words`),digits = 0), nsmall = 0)
train.data.overview$`Total Words` <- format(as.numeric(train.data.overview$`Total Words`), nsmall = 0, big.mark =",")
train.data.overview$Rows <- format(as.numeric(train.data.overview$Rows), nsmall=0, big.mark=",")
train.data.overview$Size.MB <- format(round(as.numeric(train.data.overview$Size.MB), 1), nsmall=1, big.mark=",")
formattable(train.data.overview, align = c('l', 'c', 'c', 'c', 'c'))
```
## Data Preprocessing

For this next step we will prepare our data before creating the models. The pipeline  is as follows:

        1. Remove special characters: numbes, punctuaction signs and other special characters will be removed.
        2. Lowercase: for simplicity we will use all our text in lowercase.
        3. Tokenize: each line of text will be devided int 'tokens' or individual words for analysis.
        
Other considerations:

        1. For this analysis we won't be removing 'stopwords' since we hypothesize they will be useful for predictions.
        2. We won't be stemming the words, since, again, we hypothesize that different variations of the words will preced different words.
        
```{r tokenization, eval=TRUE}
library(quanteda)
train <- read_lines("./final/en_US/train_US.txt")

train.tokens <- tokens(train, what = "word",
                       remove_numbers = TRUE, remove_punct = TRUE,
                       remove_symbols = TRUE, split_hyphens = TRUE)

train.tokens <- tokens_tolower(train.tokens)
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
```
 
## N-Gram Models

### 1-gram Model

When we tokenized our dataset, we also created our first N-gram model, a 1-gram model (or unigram), which are bassically single words. This is a first step to creating our model. We can now see things like the most frequent n-grams.
        
```{r unigram plot code, eval=TRUE}
library(ggplot2)
library(quanteda)
train.unigram.freq <- textstat_frequency(train.tokens.dfm, n = 20)
train.unigram.freq$feature <- factor(train.unigram.freq$feature,
                                     levels = train.unigram.freq$feature[order(train.unigram.freq$frequency)])



unigrams.plot <- ggplot(train.unigram.freq, 
                        aes(x = train.unigram.freq$feature,
                            y = train.unigram.freq$frequency)) +
        geom_col() + coord_flip() + labs(x = NULL, y = "Frequency")
```

```{r unigram plot fig, echo=FALSE, eval=TRUE}
unigrams.plot
```

