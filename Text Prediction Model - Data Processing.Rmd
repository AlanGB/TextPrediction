---
title: "Text Prediction Model - Analyzing the Data"
author: "Alán García Bernal"
date: "12/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## Summary

The goal of this project is to create a Text Prediction Model (for the English Language) using the [Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) given by Coursera. In this report you will find an overview of the dataset, as well as the steps taken for cleaning and processing it. Lastly, there are the N-gram Models that will be used for creating the final product.

## Data Overview

After downloading and extracting the datasets we are interested in, which are a file with **Twitter**, **Blogs** and **News** entries, we can start seeing interesting aspects of the data.

```{r data loading, cache=FALSE}
library(readr)
twitter.file <- ("./final/en_US/en_US.twitter.txt")
blogs.file <- ("./final/en_US/en_US.blogs.txt")
news.file <- ("./final/en_US/en_US.news.txt")

twitter <- read_lines(twitter.file)
blogs <- read_lines(blogs.file)
news <- read_lines(news.file)
```

Here we can see the files are fairly large, but have about the same word count. Also, as expected, Twitter is the file with more lines but less Average words, because of the character limit per twitt. Blogs is the file with more words per register, followed by news.

```{r data overvew, warning=FALSE, cache=FALSE}
library(stringi)
rows <- c("Twitter", "Blogs", "News")

Size.MB <- setNames(as.data.frame(
                        file.size(twitter.file, blogs.file, news.file)/1024,
                        row.names = rows),
                    c("Size.MB"))

Rows <- setNames(as.data.frame(c(length(twitter), length(blogs), length(news)),
                               row.names = rows),
                 c("Rows"))

Words <- setNames(as.data.frame(c(sum(stri_count_words(twitter)),
                         sum(stri_count_words(blogs)),
                         sum(stri_count_words(news))),
                         row.names= rows),
                  c("Total Words"))

Words.Ave <- setNames(as.data.frame(c(mean(stri_count_words(twitter)),
                                      mean(stri_count_words(blogs)),
                                      mean(stri_count_words(news))),
                                    row.names = rows),
                      c("Average Words"))

data.overview <- cbind(Size.MB, Rows, Words, Words.Ave)
```

```{r data overview table, echo = FALSE, warning=FALSE, cache=FALSE}
library(formattable)
data.overview$`Average Words` <- format(round(as.numeric(data.overview$`Average Words`),digits = 0), nsmall = 0)
data.overview$`Total Words` <- format(as.numeric(data.overview$`Total Words`), nsmall = 0, big.mark =",")
data.overview$Rows <- format(as.numeric(data.overview$Rows), nsmall=0, big.mark=",")
data.overview$Size.MB <- format(round(as.numeric(data.overview$Size.MB), 1), nsmall=1, big.mark=",")


formattable(data.overview,
            align = c("l", "c", "c", "c", "c"))
```

## Data Sampling

Since the dataset is so large, and due to hardware constrains, we will be using the 15% of the total data. We will join this subsets into one *corpora* and divide them to create three data sets: the **training set**, **validation set** and the **test set**. As a good practice, we will use the 70% of the *corpora* as our **training set**, 20% for our **validation set** and the other 10% as our **test set**.

For practicity we will create separte testing, validation and training files for each one of the **Twitter**, **Blogs** and **News** data sets.

#### Twitter files
```{r twitter sampling, eval=FALSE}
set.seed(123)
con <- file("./final/en_US/en_US.twitter.txt", "r")
for (i in 1:n.twitter) {
        if (rbinom(1,1,.15) == 1) {
                if (rbinom(1,1, .7) == 1) {
                        write(readLines(con,1), file = "./final/en_US/train_US.twitter.txt", append = TRUE)     
                } else {
                        if (rbinom(1,1, .2) == 1) {
                                write(readLines(con,1), file = "./final/en_US/validation_US.twitter.txt", append = TRUE)
                        } else {
                                write(readLines(con, 1), file = "./final/en_US/test_US.twitter.txt", append = TRUE)
                        }
                }
                
        }
}
```

#### Blogs files
```{r blogs sampling, eval=FALSE}
set.seed(456)
con <- file("./final/en_US/en_US.blogs.txt", "r")
for (i in 1:n.blogs) {
        if (rbinom(1,1,.15) == 1) {
                if (rbinom(1,1, .7) == 1) {
                        write(readLines(con,1), file = "./final/en_US/train_US.blogs.txt", append = TRUE)     
                } else {
                        if (rbinom(1,1, .2) == 1) {
                                write(readLines(con,1), file = "./final/en_US/validation_US.blogs.txt", append = TRUE)
                        } else {
                                write(readLines(con, 1), file = "./final/en_US/test_US.blogs.txt", append = TRUE)
                        }
                }
                
        }
}
close(con)
```

#### News files
```{r news sampling, eval=FALSE}
news <- read_lines("./final/en_US/en_US.news.txt")
set.seed(789)
for (i in 1:length(news)) {
        if(rbinom(1,1,.15) == 1) {
                if (rbinom(1,1,.7) == 1) {
                        write(news[i], file = "./final/en_US/train_US.news.txt", append = TRUE)
                } else {
                        if (rbinom(1,1,.2) == 1) {
                                write(news[i], file = "./final/en_US/validation_US.news.txt", append = TRUE)
                        } else {
                                write(news[i], file = "./final/en_US/test_US.news.txt", append = TRUE)
                        }
                }
        }
}
rm(list = "news")
```

#### Final Datasets

```{r training set, eval=FALSE, echo = FALSE}
# training file
con <- file("./final/en_US/train_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/train_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/train_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/train_US.txt", append = TRUE)
close(con)

# training file
con <- file("./final/en_US/validation_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/validation_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/validation_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/validation_US.txt", append = TRUE)
close(con)

# testing file
con <- file("./final/en_US/test_US.twitter.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/test_US.blogs.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)

con <- file("./final/en_US/test_US.news.txt", "r")
write(readLines(con), file = "./final/en_US/test_US.txt", append = TRUE)
close(con)
```

Now, we can make the same general analysis on our training set. Here we see that in all variables, except for one, we have about 10% of the original values for the complete sets. This is because we are using about 9.5% of the data. However, if we look at the average words per set, we can see it is almost the same. So we could conclude the sample set resembles the original set.

```{r training data overview, echo = FALSE}
library(stringi)
library(formattable)

twitter.file <- ("./final/en_US/train_US.twitter.txt")
blogs.file <- ("./final/en_US/train_US.blogs.txt")
news.file <- ("./final/en_US/train_US.news.txt")

twitter <- read_lines(twitter.file)
blogs <- read_lines(blogs.file)
news <- read_lines(news.file)


rows <- c("Twitter", "Blogs", "News")

Size.MB <- setNames(as.data.frame(
        file.size(twitter.file, blogs.file, news.file)/1024,
        row.names = rows),
        c("Size.MB"))

Rows <- setNames(as.data.frame(c(length(twitter), length(blogs), length(news)),
                               row.names = rows),
                 c("Rows"))

Words <- setNames(as.data.frame(c(sum(stri_count_words(twitter)),
                                  sum(stri_count_words(blogs)),
                                  sum(stri_count_words(news))),
                                row.names= rows),
                  c("Total Words"))

Words.Ave <- setNames(as.data.frame(c(mean(stri_count_words(twitter)),
                                      mean(stri_count_words(blogs)),
                                      mean(stri_count_words(news))),
                                    row.names = rows),
                      c("Average Words"))

train.data.overview <- cbind(Size.MB, Rows, Words, Words.Ave)

train.data.overview$`Average Words` <- format(round(as.numeric(train.data.overview$`Average Words`),digits = 0), nsmall = 0)
train.data.overview$`Total Words` <- format(as.numeric(train.data.overview$`Total Words`), nsmall = 0, big.mark =",")
train.data.overview$Rows <- format(as.numeric(train.data.overview$Rows), nsmall=0, big.mark=",")
train.data.overview$Size.MB <- format(round(as.numeric(train.data.overview$Size.MB), 1), nsmall=1, big.mark=",")
formattable(train.data.overview, align = c('l', 'c', 'c', 'c', 'c'))
```

## Data Preprocessing

For this next step we will prepare our data, previous to the creations of our model. The pipeline  is as follows:

        1. Remove special characters: numbes, punctuaction signs and other special characters will be removed.
        2. Lowercase: for simplicity we will use all our text in lowercase.
        3. Tokenize: each line of text will be devided int 'tokens' or individual words for analysis.
        
Other considerations:

        1. For this analysis we won't be removing 'stopwords' since we hypothesize they will be useful for predictions.
        2. We won't be stemming the words, since, again, we hypothesize that different variations of the words will preced different words.
        
```{r tokenization, eval=TRUE}
library(quanteda)
train <- read_lines("./final/en_US/train_US.txt")

train.tokens <- tokens(train, what = "word",
                       remove_numbers = TRUE, remove_punct = TRUE,
                       remove_symbols = TRUE, split_hyphens = TRUE)

train.tokens <- tokens_tolower(train.tokens)
train.tokens.dfm <- dfm(train.tokens, tolower = FALSE)
```
 
## N-Gram Models

### 1-gram Model

When we tokenized our dataset, we also created our first N-gram model, a 1-gram model (or unigram), which are bassically single words. This is a first step to creating our model. We can now see things like the most frequent n-grams.
        
```{r unigram plot code, eval=TRUE}
library(ggplot2)
library(quanteda)
train.unigram.freq <- textstat_frequency(train.tokens.dfm, n = 20)
train.unigram.freq$feature <- factor(train.unigram.freq$feature,
                                     levels = train.unigram.freq$feature[order(train.unigram.freq$frequency)])



unigrams.plot <- ggplot(train.unigram.freq, 
                        aes(x = train.unigram.freq$feature,
                            y = train.unigram.freq$frequency)) +
        geom_col() + coord_flip() + labs(x = NULL, y = "Frequency") +
        ggtitle("Top 20 Unigrams Frequency")

unigrams.plot
```

As we can see, most of the Top 20 are stopwords, which by themselves won't provide much information. Let's make this same analysis for bi-grams and tri-grams.

Here we can see that some of the Top n-grams are included in the bi-grams en tri-grams, this can be usefull for creating our prediction model.

```{r bigrams and trigrams plots, echo=FALSE}
# BIGRAMS ####
train.bigrams <- tokens_ngrams(train.tokens, n = 2)
train.bigrams.dfm <- dfm(train.bigrams)
train.bigrams.freq <- textstat_frequency(train.bigrams.dfm, n = 20)
train.bigrams.freq$feature <- factor(train.bigrams.freq$feature,
                                     levels = train.bigrams.freq$feature[order(train.bigrams.freq$frequency)])


bigrams.plot <- ggplot(train.bigrams.freq, 
                        aes(x = train.bigrams.freq$feature,
                            y = train.bigrams.freq$frequency)) +
        geom_col() + coord_flip() + labs(x = NULL, y = "Frequency") +
        ggtitle("Top 20 Bigrams Frequency")

# TRIGRAMS ####
train.trigrams <- tokens_ngrams(train.tokens, n = 3)
train.trigrams.dfm <- dfm(train.trigrams)
train.trigrams.freq <- textstat_frequency(train.trigrams.dfm, n = 20)
train.trigrams.freq$feature <- factor(train.trigrams.freq$feature,
                                     levels = train.trigrams.freq$feature[order(train.trigrams.freq$frequency)])


trigrams.plot <- ggplot(train.trigrams.freq, 
                       aes(x = train.trigrams.freq$feature,
                           y = train.trigrams.freq$frequency)) +
        geom_col() + coord_flip() + labs(x = NULL, y = "Frequency") +
        ggtitle("Top 20 Trigrams Frequency")

library(gridExtra)
grid.arrange(bigrams.plot, trigrams.plot, nrow=1)
```

## Conclusions and Next steps.

After this excersise we can make some conclusions. First, the complete dataset is too big, but with a subset of it we can work our model. Second, the n-grams models can be useful to create our algorithm. Finally, as we excluded a big part of our initial data, we need to think about how the model will handle unseen words, this will be an interesting challenge, previous to the creation of our app.
